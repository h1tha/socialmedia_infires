{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40345ad",
   "metadata": {},
   "source": [
    "# Extra Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad29f9e",
   "metadata": {},
   "source": [
    "To visualize and try to understand some of the trends in our data, we cleaned a little more and filled in some nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b9c0c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/finaldata_label.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/5nyxcmbd319fsr4s9nf8y3d00000gn/T/ipykernel_69373/3717573608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./datasets/finaldata_label.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/finaldata_label.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./datasets/finaldata_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb8a0f",
   "metadata": {},
   "source": [
    "Dropped the two extra index columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c59fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc32db4",
   "metadata": {},
   "source": [
    "After looking over the dataset, a large amount of the tweets came from California. We decided to fill the null 'user_location' values with 'California, USA'. All of the 'CA' and 'California' values were renamed to 'California, USA' because all three locations mean the same thing and there is no reason for them to be counted as separate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['user_location']] = df[['user_location']].fillna(value='CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f772027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_location'] = df['user_location'].replace('California', 'CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_location'] = df['user_location'].replace('CA', 'California, USA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb9161",
   "metadata": {},
   "source": [
    "All of the 'San Francisco' values were lumped into the 'San Francisco, CA' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2287d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_location'] = df['user_location'].replace('San Francisco', 'San Francisco, CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['verified']] = df[['verified']].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cff335",
   "metadata": {},
   "source": [
    "Checking dimensions and first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3775c",
   "metadata": {},
   "source": [
    "# EDA and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca8d7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.set_palette('crest', 9)\n",
    "sns.barplot(x = df['keyword'], y = df['relevant'])\n",
    "\n",
    "plt.title('Relevance of Tweets by Keyword', size = 14)\n",
    "plt.xlabel('Keyword', size = 11)\n",
    "plt.ylabel('Relevant Tweets out of Total', size = 11);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00d308",
   "metadata": {},
   "source": [
    "This barchart (above) shows the ratio of tweets that were deemed 'relevant' from the total tweets associated with a given keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20608c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_loc = df[['user_location']].value_counts().index.tolist()[:10]\n",
    "top_loc_counts = df[['user_location']].value_counts().tolist()[:10]\n",
    "top_loc, top_loc_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c15a2b",
   "metadata": {},
   "source": [
    "Creating a dataframe with the top 10 locations and the number of tweets from each location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['California, USA', 17629]\n",
    "list2 = ['San Francisco, CA', 13775]\n",
    "list3 = ['Sacramento, CA', 4746]\n",
    "list4 = ['Oakland, CA', 4176]\n",
    "list5 = ['San Jose, CA', 2585]\n",
    "list6 = ['United States', 1102]\n",
    "list7 = ['Los Angeles, CA', 1030]\n",
    "list8 = ['Bay Area', 940]\n",
    "list9 = ['Northern California', 839]\n",
    "list10 = ['Stockton, CA', 823]\n",
    "\n",
    "df2 = pd.DataFrame(data = [list1, list2, list3, list4, list5, list6, list7, list8, list9, list10], columns = [\"location\", \"tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0dc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.bar(x = df2['location'], height = df2['tweets'], color = 'lightgray')\n",
    "plt.xticks(rotation = -60)\n",
    "\n",
    "plt.title('Locations with the Most Tweets', size = 14)\n",
    "plt.xlabel('Location', size = 11)\n",
    "plt.ylabel('Tweet Count', size = 11);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a087",
   "metadata": {},
   "source": [
    "(Above) Chart shows the ten locations that produced the most tweets and how many tweets came from each area. Because we are interested in focusing our search efforts within a certain area around the fires, this is a way to check that our tweets are coming from where we want them to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a1f95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "sns.countplot(x=\"duplicate\", hue=\"relevant\", data=df, palette='crest');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588de0c",
   "metadata": {},
   "source": [
    "(Above) Of the tweets that appear more than once in the dataset (indicating that they are associaed with more than one keyword), just about half of them are labeled 'relevant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = df[(df['user_location'] == 'California, USA') + (df['user_location'] == 'San Francisco, CA') + (df['user_location'] == 'Sacramento, CA') + (df['user_location'] == 'Oakland, CA') + (df['user_location'] == 'San Jose, CA') + (df['user_location'] == 'United States') + (df['user_location'] == 'Los Angeles, CA') + (df['user_location'] == 'Bay Area') + (df['user_location'] == 'Northern California') + (df['user_location'] == 'Stockton, CA')]\n",
    "df_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c5f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = sns.catplot(x=\"user_location\", hue=\"relevant\", col=\"duplicate\",\n",
    "                data=df_top, kind=\"count\",\n",
    "                height=8, aspect=1, palette = 'crest')\n",
    "\n",
    "h.set_xticklabels(rotation = 90)\n",
    "h.set_xlabels('Location');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb7293",
   "metadata": {},
   "source": [
    "(Above) Basically the same thing as the previous chart, just broken down by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f083b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 8))\n",
    "sns.countplot(x=\"user_location\", hue=\"relevant\", data=df_top, palette='crest')\n",
    "plt.xticks(rotation = -45)\n",
    "\n",
    "plt.title('Tweet Relevancy by Location', size = 14)\n",
    "plt.xlabel('Location', size = 11)\n",
    "plt.ylabel('Tweet Count', size = 11);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abdc23",
   "metadata": {},
   "source": [
    "(Above) Does one location produce more relevant tweets than the others? Judging by proportions, no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d24a51",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082f4dd",
   "metadata": {},
   "source": [
    "Before training models to classify tweets as relevent or irrelevant, the training input needs to be transformed into something that can interpreted mathematically. Unstructured, unlabelled text does not mean much on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, plot_roc_curve\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from scipy.stats import uniform, loguniform\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425d73a",
   "metadata": {},
   "source": [
    "The feature used for predictions is the \"content\" column. The target is the \"relevant\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content']\n",
    "y = df['relevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b957e7",
   "metadata": {},
   "source": [
    "Split with stratify because the classes are heavily imbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2902e09",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405df3a",
   "metadata": {},
   "source": [
    "We wanted to compare model performances with both CountVectorizer and TF-IDF Vectorizer, so we processed our data both ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0e936",
   "metadata": {},
   "source": [
    "#### CVEC Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "Xc_train = cvec.fit_transform(X_train)\n",
    "Xc_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0bf97",
   "metadata": {},
   "source": [
    "#### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "Xt_train = tvec.fit_transform(X_train)\n",
    "Xt_test = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824bacf",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a444a",
   "metadata": {},
   "source": [
    "Our model needed to be a binary classifier, so we ran a number of classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184229b",
   "metadata": {},
   "source": [
    "### Logistic Regression, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = Pipeline([\n",
    "    ('cvec', CountVectorizer() ),\n",
    "    ('logit', LogisticRegression(penalty='none',\n",
    "               C = 1.0,\n",
    "               solver='lbfgs',\n",
    "               max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa20abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d6591",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = logit.score(X_train, y_train)\n",
    "test_score = logit.score(X_test, y_test)\n",
    "\n",
    "print(f'Logistic Regression model with CountVectorizer training score: {train_score}')\n",
    "print(f'Logistic Regression model with CountVectorizer testing score: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2116d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_preds = logit.predict(X_test)\n",
    "Accuracy = accuracy_score(y_test, logit_preds)\n",
    "Recall = recall_score(y_test, logit_preds)\n",
    "Precision = precision_score(y_test, logit_preds)\n",
    "F1_score = f1_score(y_test, logit_preds)\n",
    "ROC_AUC_score = roc_auc_score(y_test, logit.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'Logistic Regression model with CountVectorizer accuracy: {Accuracy}')\n",
    "print(f'Logistic Regression model with CountVectorizer recall: {Recall}')\n",
    "print(f'Logistic Regression model with CountVectorizer precision: {Precision}')\n",
    "print(f'Logistic Regression model with CountVectorizer F1 score: {F1_score}')\n",
    "print(f'Logistic Regression model with CountVectorizer AUC Score: {ROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad20ba",
   "metadata": {},
   "source": [
    "### KNN with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd37722",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', CountVectorizer(\n",
    "    max_df=.325,\n",
    "    max_features=2000,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2),\n",
    "   )),\n",
    "    ('knn', KNeighborsClassifier())])\n",
    "\n",
    "knn_params = {\n",
    "    'knn__n_neighbors': [3, 5, 7],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dac945",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn =  RandomizedSearchCV(\n",
    "    pipe,\n",
    "    knn_params,\n",
    "    n_iter=12,\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ktrain_score = knn.score(X_train, y_train)\n",
    "ktest_score = knn.score(X_test, y_test)\n",
    "\n",
    "print(f'KNN model with CountVectorizer training score: {ktrain_score}')\n",
    "print(f'KNN model with CountVectorizer testing score: {ktest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c344e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds = knn.predict(X_test)\n",
    "kAccuracy = accuracy_score(y_test, knn_preds)\n",
    "kRecall = recall_score(y_test, knn_preds)\n",
    "kPrecision = precision_score(y_test, knn_preds)\n",
    "kF1_score = f1_score(y_test, knn_preds)\n",
    "kROC_AUC_score = roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'KNN model with CountVectorizer accuracy: {kAccuracy}')\n",
    "print(f'KNN model with CountVectorizer recall: {kRecall}')\n",
    "print(f'KNN model with CountVectorizer precision: {kPrecision}')\n",
    "print(f'KNN model with CountVectorizer F1 score: {kF1_score}')\n",
    "print(f'KNN model with CountVectorizer AUC Score: {kROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b04dd",
   "metadata": {},
   "source": [
    "### KNN with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f35a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknn = KNeighborsClassifier()\n",
    "\n",
    "tknn.fit(Xt_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kttrain_score = tknn.score(Xt_train, y_train)\n",
    "kttest_score = tknn.score(Xt_test, y_test)\n",
    "\n",
    "print(f'KNN model with TF-IDF Vectorizer training score: {kttrain_score}')\n",
    "print(f'KNN model with TF-IDF Vectorizer testing score: {kttest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkknn_preds = tknn.predict(X_test)\n",
    "tkAccuracy = accuracy_score(y_test, tknn_preds)\n",
    "tkRecall = recall_score(y_test, tknn_preds)\n",
    "tkPrecision = precision_score(y_test, tknn_preds)\n",
    "tkF1_score = f1_score(y_test, tknn_preds)\n",
    "tkROC_AUC_score = roc_auc_score(y_test, tknn.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'KNN model with TF-IDF Vectorizer accuracy: {tkAccuracy}')\n",
    "print(f'KNN model with TF-IDF Vectorizer recall: {tkRecall}')\n",
    "print(f'KNN model with TF-IDF Vectorizer precision: {tkPrecision}')\n",
    "print(f'KNN model with TF-IDF Vectorizer F1 score: {tkF1_score}')\n",
    "print(f'KNN model with TF-IDF Vectorizer AUC Score: {tkROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e76e91",
   "metadata": {},
   "source": [
    "### Random Forest with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b365f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', CountVectorizer(\n",
    "    max_df=.325,\n",
    "    max_features=2000,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2),\n",
    "   )),\n",
    "     ('rf', RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "rf_params = {\n",
    "             'rf__n_estimators': [200,300,500],\n",
    "          'rf__max_depth': [20,30,50],\n",
    "          'rf__min_samples_split': [20,30,40,60],\n",
    "          'rf__min_samples_leaf': [2,4,10,20],\n",
    "          'rf__max_features': ['auto', 'sqrt']\n",
    "              }\n",
    "\n",
    "rf = RandomizedSearchCV(estimator=pipe, \n",
    "                        param_distributions = rf_params,\n",
    "                        random_state=42,\n",
    "                        cv=5)\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rftrain_score = rf.score(X_train, y_train)\n",
    "rftest_score = rf.score(X_test, y_test)\n",
    "\n",
    "print(f'Random Forest model with CountVectorizer training score: {rftrain_score}')\n",
    "print(f'Random Forest model with CountVectorizer testing score: {rftest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf.predict(X_test)\n",
    "rfAccuracy = accuracy_score(y_test, rf_preds)\n",
    "rfRecall = recall_score(y_test, rf_preds)\n",
    "rfPrecision = precision_score(y_test, rf_preds)\n",
    "rfF1_score = f1_score(y_test, rf_preds)\n",
    "rfROC_AUC_score = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'Random Forest model with CountVectorizer accuracy: {rfAccuracy}')\n",
    "print(f'Random Forest model with CountVectorizer recall: {rfRecall}')\n",
    "print(f'Random Forest model with CountVectorizer precision: {rfPrecision}')\n",
    "print(f'Random Forest model with CountVectorizer F1 score: {rfF1_score}')\n",
    "print(f'Random Forest model with CountVectorizer AUC Score: {rfROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d190738",
   "metadata": {},
   "source": [
    "### LinearSVM with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(max_iter = 2000)\n",
    "pgrid = {\"C\": np.linspace(0.0001, 1, 20)}\n",
    "gcv = GridSearchCV(svc,\n",
    "                  pgrid,\n",
    "                  cv=5,\n",
    "                  n_jobs =-1)\n",
    "\n",
    "gcv.fit(Xc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcvtrain_score = gcv.score(Xc_train, y_train)\n",
    "gcvtest_score = gcv.score(Xc_test, y_test)\n",
    "\n",
    "print(f'SVM model with CountVectorizer training score: {gcvtrain_score}')\n",
    "print(f'SVM model with CountVectorizer testing score: {gcvtest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_preds = gcv.predict(X_test)\n",
    "gcvAccuracy = accuracy_score(y_test, gcv_preds)\n",
    "gcvRecall = recall_score(y_test, gcv_preds)\n",
    "gcvPrecision = precision_score(y_test, gcv_preds)\n",
    "gcvF1_score = f1_score(y_test, gcv_preds)\n",
    "gcvROC_AUC_score = roc_auc_score(y_test, gcv.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'SVM model with CountVectorizer accuracy: {gcvAccuracy}')\n",
    "print(f'SVM model with CountVectorizer recall: {gcvRecall}')\n",
    "print(f'SVM model with CountVectorizer precision: {gcvPrecision}')\n",
    "print(f'SVM model with CountVectorizer F1 score: {gcvF1_score}')\n",
    "print(f'SVM model with CountVectorizer AUC Score: {gcvROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094db20",
   "metadata": {},
   "source": [
    "We attempted to run a RandomizedSearch over our SVM model, but it errored out due to lack of computer memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457484d6",
   "metadata": {},
   "source": [
    "### LinearSVM with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(random_state=42)\n",
    "lsvc.fit(Xt_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef290b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvctrain_score = lsvc.score(Xt_train, y_train)\n",
    "lsvctest_score = lsvc.score(Xt_test, y_test)\n",
    "\n",
    "print(f'LinearSVC model with TF-IDF Vectorizer training score: {lsvctrain_score}')\n",
    "print(f'LinearSVC model with TF-IDF Vectorizer testing score: {lsvctest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_preds = lsvc.predict(Xt_test)\n",
    "lsvcAccuracy = accuracy_score(y_test, lsvc_preds)\n",
    "lsvcRecall = recall_score(y_test, lsvc_preds)\n",
    "lsvcPrecision = precision_score(y_test, lsvc_preds)\n",
    "lsvcF1_score = f1_score(y_test, lsvc_preds)\n",
    "lsvcROC_AUC_score = roc_auc_score(y_test, lsvc.predict_proba(Xt_test)[:, 1])\n",
    "\n",
    "print(f'LinearSVC model with TF-IDF Vectorizer accuracy: {lsvcAccuracy}')\n",
    "print(f'LinearSVC model with TF-IDF Vectorizer recall: {lsvcRecall}')\n",
    "print(f'LinearSVC model with TF-IDF Vectorizer precision: {lsvcPrecision}')\n",
    "print(f'LinearSVC model with TF-IDF Vectorizer F1 score: {lsvcF1_score}')\n",
    "print(f'LinearSVC model with TF-IDF Vectorizer AUC Score: {lsvcROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51b71d",
   "metadata": {},
   "source": [
    "### SVC with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvc = SVC()\n",
    "tsvc.fit(Xt_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f666b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tstrain_score = gcv.score(Xt_train, y_train)\n",
    "tstest_score = gcv.score(Xt_test, y_test)\n",
    "\n",
    "print(f'SVM model with TF-IDF Vectorizer training score: {tstrain_score}')\n",
    "print(f'SVM model with TF-IDF Vectorizer testing score: {tstest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530157d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvc_preds = tsvc.predict(Xt_test)\n",
    "tsAccuracy = accuracy_score(y_test, tsvc_preds)\n",
    "tsRecall = recall_score(y_test, tsvc_preds)\n",
    "tsPrecision = precision_score(y_test, tsvc_preds)\n",
    "tsF1_score = f1_score(y_test, tsvc_preds)\n",
    "tsROC_AUC_score = roc_auc_score(y_test, tsvc.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'SVM model with TF-IDF Vectorizer accuracy: {tsAccuracy}')\n",
    "print(f'SVM model with TF-IDF Vectorizer recall: {tsRecall}')\n",
    "print(f'SVM model with TF-IDF Vectorizer precision: {tsPrecision}')\n",
    "print(f'SVM model with TF-IDF Vectorizer F1 score: {tsF1_score}')\n",
    "print(f'SVM model with TF-IDF Vectorizer AUC Score: {tsROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4919db5b",
   "metadata": {},
   "source": [
    "### Decision Tree with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cfe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=10,\n",
    "                            min_samples_split =7,\n",
    "                            min_samples_leaf = 3,\n",
    "                            ccp_alpha=0.01,\n",
    "                            random_state = 42)\n",
    "\n",
    "dt.fit(Xc_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dttrain_score = dt.score(Xc_train, y_train)\n",
    "dttest_score = dt.score(Xc_test, y_test)\n",
    "\n",
    "print(f'Decision Tree model with CountVectorizer training score: {dttrain_score}')\n",
    "print(f'Decision Tree model with CountVectorizer testing score: {dttest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_preds = dt.predict(X_test)\n",
    "dtAccuracy = accuracy_score(y_test, dt_preds)\n",
    "dtRecall = recall_score(y_test, dt_preds)\n",
    "dtPrecision = precision_score(y_test, dt_preds)\n",
    "dtF1_score = f1_score(y_test, dt_preds)\n",
    "dtROC_AUC_score = roc_auc_score(y_test, dt.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'Decision Tree model with CountVectorizer accuracy: {dtAccuracy}')\n",
    "print(f'Decision Tree model with CountVectorizer recall: {dtRecall}')\n",
    "print(f'Decision Tree model with CountVectorizer precision: {dtPrecision}')\n",
    "print(f'Decision Tree model with CountVectorizer F1 score: {dtF1_score}')\n",
    "print(f'Decision Tree model with CountVectorizer AUC Score: {dtROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f496a",
   "metadata": {},
   "source": [
    "This model was one of our top performers; it had the highest precision score. Because of this, we plotted the ROC/AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plot_roc_curve(dt, Xc_test, y_test)\n",
    "plt.plot([0,1], [0,1],label = 'baseline', linestyle = '--')\n",
    "plt.title('Decision Tree ROC')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd3c51",
   "metadata": {},
   "source": [
    "### TVEC with DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt = DecisionTreeClassifier(random_state = 42)\n",
    "tdt.fit(Xt_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdttrain_score = dt.score(Xt_train, y_train)\n",
    "tdttest_score = dt.score(Xt_test, y_test)\n",
    "\n",
    "print(f'Decision Tree model with TF-IDF Vectorizer training score: {tdttrain_score}')\n",
    "print(f'Decision Tree model with TF-IDF Vectorizer testing score: {tdttest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt_preds = tdt.predict(Xt_test)\n",
    "tdtAccuracy = accuracy_score(y_test, tdt_preds)\n",
    "tdtRecall = recall_score(y_test, tdt_preds)\n",
    "tdtPrecision = precision_score(y_test, tdt_preds)\n",
    "tdtF1_score = f1_score(y_test, tdt_preds)\n",
    "tdtROC_AUC_score = roc_auc_score(y_test, tdt.predict_proba(Xt_test)[:, 1])\n",
    "\n",
    "print(f'Decision Tree model with TF-IDF Vectorizer accuracy: {tdtAccuracy}')\n",
    "print(f'Decision Tree model with TF-IDF Vectorizer recall: {tdtRecall}')\n",
    "print(f'Decision Tree model with TF-IDF Vectorizer precision: {tdtPrecision}')\n",
    "print(f'Decision Tree model with TF-IDF Vectorizer F1 score: {tdtF1_score}')\n",
    "print(f'Decision Tree model with TF-IDF Vectorizer AUC Score: {tdtROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d24a3",
   "metadata": {},
   "source": [
    "### Multinomial Naive-Bayes with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7737eb6",
   "metadata": {},
   "source": [
    "Multinomial Naive-Bayes is one of the few models we were able to run a GridSearch on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([\n",
    "    ('cvec', CountVectorizer() ),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [1000, 2000, 3000, 4000, 5000],\n",
    "    'cvec__min_df': [1, 2, 3, 4,5],\n",
    "    'cvec__max_df': [0.95, 0.9, 1],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "                }\n",
    "\n",
    "grid = GridSearchCV(nb,\n",
    "                   pipe_params,\n",
    "                   cv = 5)\n",
    "\n",
    "mnb = RandomizedSearchCV(nb,\n",
    "                   pipe_params,\n",
    "                    n_iter=200,\n",
    "                              cv=5,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=42)\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "cmnb_train = mnb.score(X_train, y_train)\n",
    "cmnb_test = mnb.score(X_test, y_test)\n",
    "\n",
    "print(f'Multinomial Naive-Bayes model with CountVectorizer training score: {cmnb_train}')\n",
    "print(f'Multinomial Naive-Bayes model with CountVectorizer testing score: {cmnb_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcda921",
   "metadata": {},
   "source": [
    "These are the parameters that produced the best scores from the Multinomial Naive-Bayes (CV) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79865979",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07572b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrain_score = mnb.score(X_train, y_train)\n",
    "mtest_score = mnb.score(X_test, y_test)\n",
    "\n",
    "print(f'Decision Tree model with CountVectorizer training score: {dttrain_score}')\n",
    "print(f'Decision Tree model with CountVectorizer testing score: {dttest_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_preds = mnb.predict(X_test)\n",
    "mAccuracy = accuracy_score(y_test, mnb_preds)\n",
    "mRecall = recall_score(y_test, mnb_preds)\n",
    "mPrecision = precision_score(y_test, mnb_preds)\n",
    "mF1_score = f1_score(y_test, mnb_preds)\n",
    "mROC_AUC_score = roc_auc_score(y_test, mnb.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'Multinomial Naive-Bayes model with CountVectorizer accuracy: {mAccuracy}')\n",
    "print(f'Multinomial Naive-Bayes model with CountVectorizer recall: {mRecall}')\n",
    "print(f'Multinomial Naive-Bayes model with CountVectorizer precision: {mPrecision}')\n",
    "print(f'Multinomial Naive-Bayes model with CountVectorizer F1 score: {mF1_score}')\n",
    "print(f'Multinomial Naive-Bayes model with CountVectorizer AUC Score: {mROC_AUC_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d1a6e",
   "metadata": {},
   "source": [
    "Multinomial Naive-Bayes was another top-performing model; it had the least amount of overfitting. We created a AUC/ROC plot for this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcf67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(mnb, X_test, y_test)\n",
    "plt.plot([0,1], [0,1],label = 'baseline', linestyle = '--')\n",
    "plt.title('MNB ROC')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f15774",
   "metadata": {},
   "source": [
    "# Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d5706",
   "metadata": {},
   "source": [
    "Of the models we tested, our Multinomial Naive-Bayes and Decision Tree models had the best outputs. MNB with both types of vectorizers had very low amounts of overfitting and DT scored very high in the metrics we chose to focus on (accuracy, precision, and recall), meaning that is the best model to answer our problem statement. Each metric can be optimized depending on the situation. Accuracy is a great metric for non-technical audiences and to gain a big picture understanding of how well the model is working. Precision can be optimized in situations where it's okay if people get a few tweets that are irrelevant, like the 2020 CA wildfires. It was not really a life or death situation, people had time to prepare and decide what their next move would be, so we believe precision is appropriate here. Recall should optimized when a situation is dire and the only tweets coming through are the relevant ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
